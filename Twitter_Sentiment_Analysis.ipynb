{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\n\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.sentiment import vader\nanalyzer = vader.SentimentIntensityAnalyzer()\n\n#!pip install vaderSentiment\n#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tweets.csv\")\ndata = data[[\"Sarcasm\", \"tweet\"]]\n#data.head(10)\nprint(data.tweet[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['tweet'] = data['tweet'].str.replace('#sarcasm', '')\nprint(data.tweet[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzer = SentimentIntensityAnalyzer()\n\nfinal_list = []\nfor sent in data['tweet']:\n    senti = analyzer.polarity_scores(sent)\n    list_temp=[]\n    for key, value in senti.items():\n        temp = value\n        list_temp.append(temp)\n    final_list.append(list_temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_df = pd.DataFrame(final_list, columns=['compound','neg','neu','pos'], index=data.index)\ndata = pd.merge(data, temp_df, left_index=True,right_index=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(data, test_size=0.15, random_state=101)\ntrain_df, val_df = train_test_split(train_df, test_size=0.10, random_state=101)\nprint(\"Train size:{}\".format(train_df.shape))\nprint(\"Validation size:{}\".format(val_df.shape))\nprint(\"Test size:{}\".format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 300 \nmax_features = 50000 \nmaxlen = 100 \n\n## fill up the missing values\ntrain_X = train_df[\"tweet\"].fillna(\"_na_\").values\nval_X = val_df[\"tweet\"].fillna(\"_na_\").values\ntest_X = test_df[\"tweet\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['Sarcasm'].values\nval_y = val_df['Sarcasm'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size)(inp)\nx = Conv1D(256, maxlen)(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(32, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel2 = Model(inputs=inp, outputs=x)\nadam =  Adam(lr=0.0001,decay=0.00001)\nmodel2.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n\nprint(model2.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2.fit(train_X, train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y), \n           callbacks=[EarlyStopping(monitor='val_loss', min_delta=0, patience=15)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = model2.predict([val_X], batch_size=512, verbose=1)\ny_pred2 = y_pred2>0.4\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nprint(\"Accuracy Score: \", accuracy_score(val_y, y_pred2))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(val_y, y_pred2))\nprint(\"F1 Score: \", metrics.f1_score(val_y, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_X = data[\"tweet\"].fillna(\"_na_\").values\ndata_X = tokenizer.texts_to_sequences(data_X)\ndata_X = pad_sequences(data_X, maxlen=maxlen)\n\ny_pred_data2 = model2.predict([data_X], batch_size=512, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = pd.DataFrame(y_pred_data2,columns=['CNN'], index=data.index)\n\ndata = pd.merge(data, d, left_index=True,right_index=True)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_X = data[['compound','neg','neu','pos','CNN']]\ntemp_y = data['Sarcasm']\n\nX_train, X_test, y_train, y_test = train_test_split(temp_X,temp_y, test_size=0.33, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nparams = {'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric': {'l2', 'l1'},\n          'num_leaves': 100,\n          'learning_rate': 0.1,\n          'feature_fraction': 0.9,\n          'bagging_fraction': 0.8,\n          'bagging_freq': 5,\n          'verbose': 1\n         }\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=500,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_lgb = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred_lgb = y_pred_lgb>0.4\n\ns=metrics.f1_score(y_test, y_pred_lgb)\nss=accuracy_score(y_test, y_pred_lgb)\nprint(\"Accuracy Score: \", accuracy_score(y_test, y_pred_lgb))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred_lgb))\nprint(\"F1 Score: \", metrics.f1_score(y_test, y_pred_lgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(gamma='auto')\n\ncv = KFold(n_splits=10, random_state=42, shuffle=True)\nscores = []\ni = 1\nfor train_index, test_index in cv.split(temp_X):\n    \n    X_train, X_test =  temp_X.values[train_index], temp_X.values[test_index]\n    y_train, y_test = temp_y[train_index], temp_y[test_index]\n    \n    svc.fit(X_train, y_train)\n    print(\"Iteration: \",i,\" - Score = \",svc.score(X_test, y_test).round(3))\n    scores.append(svc.score(X_test, y_test))\n    i+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_svc = svc.predict(temp_X)\n\nsss=metrics.f1_score(temp_y, y_pred_svc)\nssss=accuracy_score(temp_y, y_pred_svc)\nprint(\"Accuracy Score: \", accuracy_score(temp_y, y_pred_svc))\nprint(\"Confusion Matrix: \\n\", confusion_matrix(temp_y, y_pred_svc))\nprint(\"F1 Score: \", metrics.f1_score(temp_y, y_pred_svc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=[s,sss]\n\nlabel=['lightGBM','SVM']\nindex = np.arange(len(label))\nplt.bar(index, score)\nplt.xlabel('Algorithms', fontsize=12)\nplt.ylabel('score', fontsize=12)\nplt.xticks(index, label, fontsize=10, rotation=0)\nplt.title('F1-Score plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=[ss,ssss]\n\nlabel=['lightGBM','SVM']\nindex = np.arange(len(label))\nplt.bar(index, score)\nplt.xlabel('Algorithms', fontsize=12)\nplt.ylabel('accuracy', fontsize=12)\nplt.xticks(index, label, fontsize=10, rotation=0)\nplt.title('Accuracy Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"       ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}